# ケラスのQ学習

[farizrahman4u/qlearning4k](https://github.com/farizrahman4u/qlearning4k) を少し改変しました。
![](https://github.com/PonDad/qgakusyuu4k/blob/master/catch.gif)
![](https://github.com/PonDad/qgakusyuu4k/blob/master/snake.gif)

以下意訳です。

## 強化学習101：

強化学習とは、（ゲームのスコアを最大限に）量を最適化するために、環境内で特定のアクションを実行すること（コントローラーのボタンを押すこと）と、起こるそれが行うすべての行動に対して、エージェントは環境から肯定的、否定的または報酬ゼロを得る。これらの報酬は、エージェントがその行動が環境に対してどのような影響をもたらしたかを理解する助けとなり、エージェントはより高い累積報酬をもたらすような行動をすることを学ぶ。

グーグルDeepMindのディープQ-学習がアタリブレイクアウトを再生する：あなたはおそらくすでにルックを持っていない場合はDeepMindのQ-learingモデルは、上司のようなアタリブレイクアウトを再生見ているだろう。関連の論文は、「リファレンス」の下に見つけることができますが、PDFの男（またはギャル..（冗談（私が意味する、オッズは何ですか？））でない場合、私は、トピックに関するこのナバナのポストを示唆しています。

今、数学をすることができます。

簡単にするために、Gをゲーム状態Sとアクションaを入力とし、新しい状態S 'と報酬rを出力する関数とする。 Sはおそらくゲームの状態を表す数字の束です。 aはアクションであり、例えばゲームコントローラ上のボタンを表す数字である。

「ゲーム盤（状態, アクション）= [次の状態、報酬]」

2つの値を返す関数？ええ、それを打ち破ろう。

「ゲーム盤の状態（状態, アクション）= 次の状態」

「ゲーム盤の報酬（状態, アクション）= 報酬」

きちんとした

理想的には、与えられた状態に対して最良の行動を返す関数F（S）が必要である。

「最良の行動（状態）= アクション」

しかし、ニューラルネットワークモデルの構造は異なっており、与えられた状態に対する各行動（Q値と呼ばれ、したがってQ学習という用語）の期待最大スコアを返す：

「ニューラルネットワークモデル（状態）= {q1、q2、q3、... qn}」

ここで、q1、q2、...、qnは、可能なアクションa1、a2、... anのそれぞれについての期待最大スコア（Q値）である。エージェントは単に最高報酬で行動します。このレポのコンテキストでは、MはちょうどSを入力として取り、Q値を出力するKerasモデルです。モデルによって出力されるQ値の数は、ゲーム内の可能なアクションの数に等しい。キャッチの試合では、左に行ったり、右に行ったり、ただ単に留まることができます。 3つのアクション、つまり3つのQ値を意味し、各アクションに1つ。

ここで、必要な関数Fをニューラルネットワークモデルで再定義することができます：

「最良の行動（状態）= 最大値（ニューラルネットワークモデル（状態））」

クール！

ここで、これらのQ値をモデルから訓練するために、どこから得ますか？ Q関数から。

公式にQ関数を定義します：

Q（S、A）=最大スコアあなたのエージェントは「ゲームは状態Sのときに、彼は行動aをした場合我々は行動aを実行するには、ゲームは新しい状態Sにジャンプしますことを知って、ゲームの終わりで取得しますエージェントに即座の報酬rを与える。

「次の状態= ゲーム盤の状態（状態, アクション）」

「報酬 = ゲーム盤の報酬（状態, アクション）」

ゲームが状態S 'にあるとき、エージェントはそのニューラルネットワークモデルMに従って行動a'を行うことに留意されたい。

「次のアクション= 最良の行動（状態）= 最大値（ニューラルネットワークモデル（次の状態）））」

Qを定義する時間、もう一度：

Q（S、a）=この状態からの報酬+次の状態からの最大スコア

正式には：

｢Q（状態、アクション）= 報酬 + Q（次の状態、次のアクション'）｣

うーん、再帰関数。

今や、割引係数、ガンマを導入することによって、即時報酬に大きな重みを付けることができます：

「Q（状態、アクション）= 報酬 + 割引係数 x Q（次の行動、次のアクション）」

あなたはあなたのSとあなたのQを手に入れました。あなたのモデルを訓練するだけです。
