## Ｑ学習４Ｋ

Pythonディープ・ラーニング・ライブラリのKerasの補強学習アドオンQlearning4kを少し改変しました。

[farizrahman4u/qlearning4k](https://github.com/farizrahman4u/qlearning4k)

以下、ＲＥＡＤＭＥの意訳です。

### 強化学習101：

強化学習とは、（ゲームのスコアを最大限に）量を最適化するために、環境内で特定のアクションを実行すること（コントローラーのボタンを押すこと）と、起こるそれが行うすべての行動に対して、エージェントは環境から肯定的、否定的または報酬ゼロを得る。これらの報酬は、エージェントがその行動が環境に対してどのような影響をもたらしたかを理解する助けとなり、エージェントはより高い累積報酬をもたらすような行動をすることを学ぶ。

今、数学をすることができます。

簡単にするために、Gをゲーム状態Sとアクションaを入力とし、新しい状態S 'と報酬rを出力する関数とする。 Sはおそらくゲームの状態を表す数字の束です。 aはアクションであり、例えばゲームコントローラ上のボタンを表す数字である。

**ゲーム盤（状態、行動）= [次の状態、報酬]**

2つの値を返す関数？ええ、それを打ち破ろう。

**ゲーム盤の状態（状態、行動）= 次の状態**

**ゲーム盤の報酬（状態、行動）= 報酬**

きちんとした

理想的には、与えられた状態に対して最良の行動を返す関数F（S）が必要である。

**最良の行動（状態）= 行動**

しかし、ニューラルネットワークモデルの構造は異なっており、与えられた状態に対する各行動（Q値と呼ばれ、したがってQ学習という用語）の期待最大スコアを返す：

**ニューラルネットワークモデル（状態）= {Q値1、Q値2、Q値3、... Q値n}**

ここで、q1、q2、...、qnは、可能なアクションa1、a2、... anのそれぞれについての期待最大スコア（Q値）である。エージェントは単に最高報酬で行動します。このレポのコンテキストでは、MはちょうどSを入力として取り、Q値を出力するKerasモデルです。モデルによって出力されるQ値の数は、ゲーム内の可能なアクションの数に等しい。キャッチの試合では、左に行ったり、右に行ったり、ただ単に留まることができます。 3つのアクション、つまり3つのQ値を意味し、各アクションに1つ。

ここで、必要な関数FをニューラルネットワークMで再定義することができます：

**最良の行動（状態）= 最大値（ニューラルネットワークモデル（状態））**

クール！

ここで、これらのQ値をモデルから訓練するために、どこから得ますか？ Q関数から。

公式にQ関数を定義します：

Q（S、A）=最大スコアあなたのエージェントは「ゲームは状態Sのときに、彼は行動aをした場合我々は行動aを実行するには、ゲームは新しい状態Sにジャンプしますことを知って、ゲームの終わりで取得しますエージェントに即座の報酬rを与える。

**次の状態= ゲーム盤の報酬（状態、行動）**

**報酬 = ゲーム盤の報酬（状態、行動）**

ゲームが状態S 'にあるとき、エージェントはそのニューラルネットワークモデルMに従って行動a'を行うことに留意されたい。
次の行動= 最良の行動（次の状態）= 最大値（ニューラルネットワークモデル（次の状態））

Qを定義する時間、もう一度：

Q（S、a）=この状態からの報酬次の状態からの最大スコア

正式には：

**Q（状態, 行動）= 報酬 Q（次の状態, 次の行動）**

うーん、再帰関数。

今や、割引係数、ガンマを導入することによって、即時報酬に大きな重みを付けることができます：

**Q（状態, 行動）=報酬　割引率* Q（次の状態, 次の行動）**

あなたはあなたのSとあなたのQを手に入れました。あなたのモデルを訓練するだけです。
